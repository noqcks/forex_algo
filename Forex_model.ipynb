{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "### STEPS ###\n",
    "# 1. tokenize text\n",
    "# 2. remove stopwords\n",
    "# 3. discard if words not in wordnet\n",
    "# 4. discard if words first hypernym not available\n",
    "# 5. create feature vector of word hypernyms\n",
    "# 6. assign tfidf weights to feature vector\n",
    "# 7. calculate sentiment sumscores on words\n",
    "# 8. multiply sumscore vector by tfidf weights vector\n",
    "# 9. store tfidf feature vector\n",
    "# 10. store tfidf*sumscore vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device.n.01 activity.n.01 commercial_enterprise.n.01 investigation.n.02 magnitude.n.01 assets.n.01 organic_process.n.01 direction.n.01 prediction.n.02 pipe.n.04 activity.n.01 commercial_enterprise.n.01 investigation.n.02 magnitude.n.01 assets.n.01 organic_process.n.01 direction.n.01 prediction.n.02 metallic_element.n.01 activity.n.01 instrumentality.n.03 commercial_enterprise.n.01 investigation.n.02 magnitude.n.01 assets.n.01 organic_process.n.01 direction.n.01 prediction.n.02 beverage.n.01 lipid.n.01 dairy_product.n.01 lipid.n.01 activity.n.01 commercial_enterprise.n.01 direction.n.01 assets.n.01 magnitude.n.01 organic_process.n.01 possibility.n.02 prediction.n.02 device.n.01 activity.n.01 commercial_enterprise.n.01 investigation.n.02 magnitude.n.01 assets.n.01 organic_process.n.01 direction.n.01 prediction.n.02 way.n.06 inform.v.01 evaluation.n.02 letter.n.02 people.n.01 physical_phenomenon.n.01 representational_process.n.01 imaging.n.02 instrumentality.n.03 activity.n.01 prediction.n.02 wave.n.03 opposition.n.05 grasping.n.02 time_period.n.01 extremity.n.04 meeting.n.01 establish.v.02 corporate_executive.n.01 inform.v.01 modify.v.01 capitalist.n.02 computer.n.01 financial_gain.n.01 money.n.01 arrangement.n.03 activity.n.01 investigation.n.02 improvement.n.02 prediction.n.02 setting.n.02 metallic_element.n.01 metallic_element.n.01 station.n.01 closet.n.04 workplace.n.01 experiment.n.01 army_unit.n.01 time_unit.n.01 work.n.01 definite_quantity.n.01 travel.v.01 agency.n.01 firm.n.01 document.n.01 rank.n.02 common_fraction.n.01 phase_of_the_moon.n.01 time_period.n.01 commercial_enterprise.n.02 activity.n.01 commercial_enterprise.n.01 investigation.n.02 magnitude.n.01 assets.n.01 organic_process.n.01 direction.n.01 prediction.n.02 activity.n.01 examination.n.01 injury.n.01 operation.n.06 commodity.n.01 lesion.n.01 use.n.01 part.n.02 hold.v.11 bodily_property.n.01 end.n.02 enlisted_man.n.01 position.n.07 gathering.n.01 oxide.n.01 colloid.n.01 activity.n.01 investigation.n.02 use.n.01 chemical_agent.n.01 agent.n.03 traveler.n.01 agent.n.03 fluid.n.01 agent.n.03 sorbent.n.01 location.n.01 part.n.02 prediction.n.02 legal_document.n.01 share.n.01 activity.n.01 change.v.01 meal.n.01\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.read_csv('news/MarketWatch-Combined_pn.csv')\n",
    "\n",
    "### 1. TOKENIZE TEXT ###\n",
    "### 2. REMOVE STOPWORDS ###\n",
    "\"\"\"\n",
    "This is where we remove punctuation and whitespace and also stop words\n",
    "that have low informational content.\n",
    "\"\"\"\n",
    "# loading stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_text(headline):\n",
    "    # removing stopwords\n",
    "    arr = [i for i in nltk.word_tokenize(headline.lower()) if i not in stop]\n",
    "    \n",
    "    # remove symbols and numbers\n",
    "    str = \" \".join(arr)\n",
    "    str = re.sub('[^A-Za-z\\s]+', '', str)\n",
    "    str = re.sub(\"\\s\\s+\", \" \", str)\n",
    "    \n",
    "    return str\n",
    "\n",
    "news_df['headline'] = news_df['headline'].apply(normalize_text)\n",
    "# print(news_df['headline'])\n",
    "\n",
    "### 3. DISCARD IF NOT IN WORDNET ###\n",
    "### 4. DISCARED IF HYPERNYM NOT AVAILABLE ###\n",
    "### 5. CREATE FEATURE VECTOR OF WORD HYPERNYMS ###\n",
    "\n",
    "# replace words with hypernyms\n",
    "def hypernym_replace(headline):\n",
    "    arr = []\n",
    "    for word in headline.split():\n",
    "        # check if word exists in wordnet\n",
    "        if wn.synsets(word) != []:\n",
    "            # check if word has hypernyms\n",
    "            if wn.synsets(word)[0].hypernyms() != []:\n",
    "                arr.append(wn.synsets(word)[0].hypernyms()[0]._name)\n",
    "    array = \" \".join(arr)\n",
    "    return array\n",
    "\n",
    "news_df['headline'] = news_df['headline'].apply(hypernym_replace)\n",
    "# print(news_df['headline'])\n",
    "\n",
    "### 5. CREATE FEATURE VECTOR OF WORD HYPERNYMS ###\n",
    "### 6. ASSIGN TFIDF WEIGHTS TO FEATURE VECTOR ###\n",
    "vectorizer = TfidfVectorizer()\n",
    "news_df['tfidf'] = vectorizer.fit_transform(news_df['headline'])\n",
    "print(news_df['headline'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 545)\t0.0\n",
      "  (0, 30)\t0.0401676632198\n",
      "  (0, 368)\t0.0\n",
      "  (0, 1002)\t0.034244474494\n",
      "  (0, 1100)\t0.0\n",
      "  (0, 149)\t0.0524444131482\n",
      "  (0, 1287)\t0.0\n",
      "  (0, 558)\t0.0\n",
      "  (0, 1416)\t0.0\n",
      "  (0, 1361)\t0.0\n",
      "  (0, 1154)\t0.0\n",
      "  (0, 983)\t0.0\n",
      "  (0, 213)\t0.0154912410027\n",
      "  (0, 1079)\t0.0\n",
      "  (0, 492)\t0.0\n",
      "  (0, 1405)\t0.0109684669532\n",
      "  (0, 1919)\t0.0\n",
      "  (0, 965)\t0.0\n",
      "  (0, 677)\t0.0\n",
      "  (0, 1057)\t0.0\n",
      "  (0, 1333)\t0.0\n",
      "  (0, 1355)\t0.0\n",
      "  (0, 1541)\t0.0\n",
      "  (0, 922)\t0.0\n",
      "  (0, 1918)\t0.0\n",
      "  :\t:\n",
      "  (203, 869)\t0.0\n",
      "  (204, 30)\t0.0111158849423\n",
      "  (204, 965)\t0.0\n",
      "  (204, 131)\t0.0\n",
      "  (204, 1685)\t0.0\n",
      "  (204, 942)\t0.0\n",
      "  (204, 1153)\t0.0\n",
      "  (204, 6)\t0.0\n",
      "  (204, 1543)\t0.0\n",
      "  (204, 1059)\t0.0\n",
      "  (204, 601)\t0.0\n",
      "  (204, 1326)\t0.0\n",
      "  (204, 41)\t0.0\n",
      "  (204, 434)\t0.216747767934\n",
      "  (204, 819)\t0.0\n",
      "  (204, 1112)\t0.0\n",
      "  (204, 436)\t0.0\n",
      "  (204, 1446)\t0.0\n",
      "  (204, 633)\t0.246252724152\n",
      "  (204, 95)\t0.0\n",
      "  (204, 298)\t0.0\n",
      "  (204, 1041)\t0.0\n",
      "  (204, 96)\t0.0\n",
      "  (204, 464)\t0.0\n",
      "  (204, 891)\t0.0\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "### 7. CALCULATE SENTIMENT SUMSCORES ON WORDS ###\n",
    "# create a one-hot encoding feature vector of word sentiments\n",
    "# sentiment sum score --> sentiment_sum = pos_sentiment_score + neg_sentiment_score\n",
    "\n",
    "def remove_periods(df):\n",
    "    arr = []\n",
    "    for word in df.split():\n",
    "        the_real = word.split('.', 1)[0]\n",
    "        arr.append(the_real)\n",
    "    return \" \".join(arr)\n",
    "\n",
    "test_df = news_df['headline'].apply(remove_periods)\n",
    "vectorizer = TfidfVectorizer()\n",
    "news = vectorizer.fit_transform(test_df)\n",
    "\n",
    "# tfidf * sentiment_sum_score\n",
    "cx = scipy.sparse.coo_matrix(news)\n",
    "for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "    # find original hypernyms\n",
    "    regex=re.compile(\"%s.*\" % vectorizer.get_feature_names()[j])\n",
    "    word = [m.group(0) for l in news_df['headline'][i].split() for m in [regex.search(l)] if m][0]\n",
    "    \n",
    "    # get sum sentiment score\n",
    "    try:\n",
    "        breakdown = swn.senti_synset(word)\n",
    "        sum_sentiment = breakdown.pos_score() + breakdown.neg_score()\n",
    "    except:\n",
    "        sum_sentiment = 0\n",
    "    tfidf_score = news[i,j]\n",
    "    \n",
    "    # update original csr_matrix\n",
    "    news[i,j] = tfidf_score  * sum_sentiment\n",
    "\n",
    "# this is the tfidf * sentiment_sum_score matrix\n",
    "# print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop all zero values from the csr matrix\n",
    "news = news[news.getnnz(1)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 4)\n",
      "(119, 4)\n"
     ]
    }
   ],
   "source": [
    "# drop all rows in original df if they don't exist in the tfidf*senti matrix\n",
    "\n",
    "print(news_df.shape)\n",
    "del_array = []\n",
    "\n",
    "for index, row in news_df.iterrows():\n",
    "    try:\n",
    "        news[index]\n",
    "    except:\n",
    "        del_array.append(index)\n",
    "        \n",
    "news_df = news_df.drop(news_df.index[del_array])\n",
    "\n",
    "print(news_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 1970)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(news_df.shape)\n",
    "# print(news_df)\n",
    "# print(news)\n",
    "\n",
    "news_df['senti_tfidf'] = news\n",
    "news_df['label'].isnull().sum()\n",
    "\n",
    "news = news[pd.notnull(news_df['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning at 2017-04-13 11:08:54.496675\n",
      "Stop learning 2017-04-13 11:08:54.498748\n",
      "Elapsed learning 0:00:00.002073\n",
      "15\n",
      "Classification report for classifier SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.05, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N       0.00      0.00      0.00         9\n",
      "          P       0.40      1.00      0.57         6\n",
      "\n",
      "avg / total       0.16      0.40      0.23        15\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[0 9]\n",
      " [0 6]]\n",
      "Accuracy=0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# split data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import datetime as dt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(news, news_df['label'], test_size=0.15, random_state=42)\n",
    "\n",
    "# train our model\n",
    "param_C = 5\n",
    "param_gamma = 0.05\n",
    "classifier = svm.SVC(C=param_C,gamma=param_gamma)\n",
    "\n",
    "start_time = dt.datetime.now()\n",
    "print('Start learning at {}'.format(str(start_time)))\n",
    "classifier.fit(X_train, y_train)\n",
    "end_time = dt.datetime.now()\n",
    "print('Stop learning {}'.format(str(end_time)))\n",
    "elapsed_time= end_time - start_time\n",
    "print('Elapsed learning {}'.format(str(elapsed_time)))\n",
    "\n",
    "# get some accuracy\n",
    "expected = y_test\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % cm)\n",
    "\n",
    "print(\"Accuracy={}\".format(metrics.accuracy_score(expected, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
